# meta_collective/tool.py
"""
Tool Layer (z=0.867)
====================

The Tool layer wraps the Internal Model with action capabilities.

    ┌─────────────────────────────────────────────────────┐
    │           TOOL (z=0.867)                             │
    │  ┌──────────────────────────────────────────────┐   │
    │  │ Internal Model (Kaelhedron + Luminahedron)   │   │
    │  │     κ-field   │   λ-field                    │   │
    │  └──────────────────────────────────────────────┘   │
    │                                                      │
    │  Actions:                                            │
    │    - Sense: Receive observations                     │
    │    - Predict: Generate predictions                   │
    │    - Act: Execute actions based on policy            │
    │    - Learn: Update internal model                    │
    └─────────────────────────────────────────────────────┘

The Tool minimizes free energy through:
1. Perceptual inference (updating beliefs)
2. Active inference (acting to fulfill predictions)
"""

from __future__ import annotations

import math
import uuid
from dataclasses import dataclass, field
from enum import Enum
from typing import Any, Callable, Dict, List, Optional, Tuple

from .internal_model import InternalModel, Prediction, PredictionError
from .free_energy import (
    FreeEnergyMinimizer,
    HierarchicalMinimizer,
    Precision,
    VariationalState,
)

# Constants
PHI = (1 + math.sqrt(5)) / 2
PHI_INV = 1 / PHI
TAU = 2 * math.pi

# Tool z-level
Z_TOOL = 0.867


class ToolState(Enum):
    """Operating states of the Tool."""
    IDLE = "idle"
    SENSING = "sensing"
    PREDICTING = "predicting"
    ACTING = "acting"
    LEARNING = "learning"


class ActionType(Enum):
    """Types of actions a Tool can execute."""
    NULL = "null"               # No action
    EXPLORE = "explore"         # Reduce uncertainty
    EXPLOIT = "exploit"         # Maximize expected reward
    COMMUNICATE = "communicate" # Share information with other Tools


@dataclass
class Action:
    """An action generated by the Tool's policy."""
    action_type: ActionType
    value: float                # Action magnitude
    direction: int              # Action direction (-1, 0, +1)
    confidence: float           # Confidence in action
    expected_outcome: float     # Expected observation after action
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class Policy:
    """
    Action policy based on expected free energy.

    Actions are selected to minimize expected free energy:
    G = E_q[ln q(s|π) - ln p(o,s|π)]
      = ambiguity + risk
    """
    exploration_weight: float = PHI_INV      # Weight on information gain
    exploitation_weight: float = 1 - PHI_INV  # Weight on goal achievement

    # Policy parameters
    temperature: float = 1.0                  # Softmax temperature
    action_precision: float = 1.0             # Precision of action selection

    def select_action(
        self,
        prediction: Prediction,
        target: Optional[float] = None,
        uncertainty: float = 1.0
    ) -> Action:
        """
        Select action based on expected free energy minimization.

        High uncertainty → EXPLORE (reduce ambiguity)
        Low uncertainty + target → EXPLOIT (achieve goal)
        Otherwise → NULL (maintain current state)
        """
        # Compute information gain (exploration value)
        info_gain = uncertainty * self.exploration_weight

        # Compute goal-directed value (exploitation)
        if target is not None:
            goal_value = -abs(prediction.value - target) * self.exploitation_weight
        else:
            goal_value = 0.0

        # Expected free energy for each action type
        G_explore = -info_gain  # Lower is better
        G_exploit = -goal_value if target else float('inf')
        G_null = 0.0

        # Select action with lowest expected free energy
        min_G = min(G_explore, G_exploit, G_null)

        if min_G == G_explore and uncertainty > 0.5:
            action_type = ActionType.EXPLORE
            value = uncertainty
            direction = 1 if prediction.value < 0 else -1
        elif min_G == G_exploit and target is not None:
            action_type = ActionType.EXPLOIT
            value = abs(prediction.value - target)
            direction = 1 if target > prediction.value else -1
        else:
            action_type = ActionType.NULL
            value = 0.0
            direction = 0

        confidence = math.exp(-min_G / max(self.temperature, 0.01))

        return Action(
            action_type=action_type,
            value=value,
            direction=direction,
            confidence=confidence,
            expected_outcome=prediction.value + direction * value,
        )


class Tool:
    """
    Tool layer wrapping Internal Model with action capabilities.

    The Tool is the atomic unit of active inference in the architecture.
    It contains an Internal Model (Kaelhedron + Luminahedron) and
    implements the perception-action loop.

    z-level: 0.867 (between EMERGENCE and PERSISTENCE)
    """

    def __init__(
        self,
        tool_id: Optional[str] = None,
        z_level: float = Z_TOOL
    ):
        self.tool_id = tool_id or f"tool_{uuid.uuid4().hex[:8]}"
        self.z_level = z_level
        self.state = ToolState.IDLE

        # Internal model
        self.internal_model = InternalModel(
            model_id=f"{self.tool_id}_internal",
            z_level=z_level - 0.067  # Slightly lower z for internal model
        )

        # Free energy minimizer for the Tool level
        self.minimizer = HierarchicalMinimizer(z_level, n_states=3)
        self.minimizer.add_child(self.internal_model.minimizer)

        # Action policy
        self.policy = Policy()

        # Observation buffer
        self._observations: List[float] = []
        self._max_observations = 100

        # Action history
        self._actions: List[Action] = []
        self._max_actions = 100

        # Statistics
        self._total_cycles = 0
        self._cumulative_free_energy = 0.0

        # Parent triad reference
        self._parent_triad: Optional[Any] = None

    @property
    def free_energy(self) -> float:
        """Total free energy including internal model."""
        return self.minimizer.hierarchical_free_energy()

    @property
    def coherence(self) -> float:
        """Coherence from internal model's dual field."""
        return self.internal_model.coherence()

    def sense(self, observation: float) -> PredictionError:
        """
        Receive an observation from the environment.

        This triggers:
        1. Prediction error computation
        2. Internal model update
        3. Free energy recalculation
        """
        self.state = ToolState.SENSING

        # Store observation
        if len(self._observations) >= self._max_observations:
            self._observations.pop(0)
        self._observations.append(observation)

        # Pass to internal model
        error = self.internal_model.observe(observation)

        # Update Tool-level free energy
        self.minimizer.minimize_step([observation])
        self._cumulative_free_energy += self.free_energy

        self.state = ToolState.IDLE
        return error

    def predict(self, context: Optional[Dict] = None) -> Prediction:
        """Generate a prediction from the internal model."""
        self.state = ToolState.PREDICTING
        prediction = self.internal_model.generate_prediction(context)
        self.state = ToolState.IDLE
        return prediction

    def act(self, target: Optional[float] = None) -> Action:
        """
        Select and return an action based on current beliefs.

        Actions are selected to minimize expected free energy.
        """
        self.state = ToolState.ACTING

        # Generate prediction
        prediction = self.predict()

        # Compute uncertainty from precision
        uncertainty = 1.0 / max(self.internal_model.precision.value, 0.01)

        # Select action
        action = self.policy.select_action(
            prediction=prediction,
            target=target,
            uncertainty=uncertainty
        )

        # Store action
        if len(self._actions) >= self._max_actions:
            self._actions.pop(0)
        self._actions.append(action)

        self.state = ToolState.IDLE
        return action

    def learn(self, observation: float, action: Optional[Action] = None) -> float:
        """
        Learn from observation (and optionally action outcome).

        Returns the prediction error magnitude.
        """
        self.state = ToolState.LEARNING

        # Process observation
        error = self.sense(observation)

        # If action was taken, update policy based on outcome
        if action is not None:
            outcome_error = observation - action.expected_outcome
            # Adjust policy parameters based on outcome
            if abs(outcome_error) > 0.5:
                # Increase exploration if predictions were poor
                self.policy.exploration_weight = min(1.0, self.policy.exploration_weight + 0.01)
            else:
                # Increase exploitation if predictions were good
                self.policy.exploitation_weight = min(1.0, self.policy.exploitation_weight + 0.01)

            # Normalize weights
            total = self.policy.exploration_weight + self.policy.exploitation_weight
            self.policy.exploration_weight /= total
            self.policy.exploitation_weight /= total

        self._total_cycles += 1
        self.state = ToolState.IDLE

        return error.magnitude

    def perception_action_cycle(
        self,
        observation: float,
        target: Optional[float] = None
    ) -> Tuple[PredictionError, Action]:
        """
        Complete one perception-action cycle.

        1. Sense observation
        2. Update internal model
        3. Select action
        4. Return error and action
        """
        # Perception: update beliefs
        error = self.sense(observation)

        # Action: select based on updated beliefs
        action = self.act(target)

        return error, action

    def communicate(self, message: Any) -> Dict:
        """
        Process incoming communication from other Tools.

        Returns response to be sent back.
        """
        # Extract pattern if message is a prediction
        if isinstance(message, Prediction):
            # Treat as observation
            self.sense(message.value)
            return {
                "received": True,
                "source": self.tool_id,
                "own_prediction": self.predict().value,
            }

        return {"received": True, "source": self.tool_id}

    def get_contribution_to_parent(self) -> Dict:
        """
        Compute contribution to parent Triad's free energy.

        This includes:
        - Current free energy
        - Coherence
        - Prediction confidence
        """
        prediction = self.predict()
        return {
            "tool_id": self.tool_id,
            "free_energy": self.free_energy,
            "coherence": self.coherence,
            "prediction_value": prediction.value,
            "prediction_confidence": prediction.confidence,
            "z_level": self.z_level,
        }

    def snapshot(self) -> Dict:
        """Return complete Tool state snapshot."""
        return {
            "tool_id": self.tool_id,
            "z_level": self.z_level,
            "state": self.state.value,
            "internal_model": self.internal_model.snapshot(),
            "free_energy": self.free_energy,
            "coherence": self.coherence,
            "policy": {
                "exploration_weight": self.policy.exploration_weight,
                "exploitation_weight": self.policy.exploitation_weight,
                "temperature": self.policy.temperature,
            },
            "total_cycles": self._total_cycles,
            "mean_free_energy": self._cumulative_free_energy / max(1, self._total_cycles),
            "n_observations": len(self._observations),
            "n_actions": len(self._actions),
        }

    def reset(self) -> None:
        """Reset Tool to initial state."""
        self.internal_model.reset()
        self.minimizer = HierarchicalMinimizer(self.z_level, n_states=3)
        self.minimizer.add_child(self.internal_model.minimizer)
        self.policy = Policy()
        self._observations.clear()
        self._actions.clear()
        self._total_cycles = 0
        self._cumulative_free_energy = 0.0
        self.state = ToolState.IDLE
